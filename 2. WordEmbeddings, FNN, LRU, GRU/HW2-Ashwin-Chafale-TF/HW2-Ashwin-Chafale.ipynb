{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# CSCI-544 Homework Assignment No. 2\n",
    "### Name : Ashwin Chafale\n",
    "### USC ID : 1990624801"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "cPmYj3mo_zsp",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eO4fCN_r96LH",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1. Dataset Generation\n",
    "- [Amazon reviews dataset](https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Jewelry_v1_00.tsv.gz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I_-_z7yf_zsr",
    "outputId": "249d9648-fc2e-408e-d911-6a283a553ef3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1766748, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"amazon_reviews_us_Jewelry_v1_00.tsv\", sep='\\t', header=0, on_bad_lines='skip')\n",
    "df = df[['review_body','star_rating']]\n",
    "df = df.dropna()\n",
    "df = df.reset_index(drop=True)\n",
    "df['star_rating'] = df['star_rating'].astype(int)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XjrEbb_t_zsr",
    "outputId": "e762cfb7-60f3-4ab3-912c-92f0d18da234",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    1080871\n",
       "4     270424\n",
       "3     159654\n",
       "1     155002\n",
       "2     100797\n",
       "Name: star_rating, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['star_rating'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "83va4sHZ-LTt",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### i. Down-sample 5-star & 4-star reviews, Up-sample 3-star, 2-star, 1-star reviews to get 100K balance dataset\n",
    "Reference : https://elitedatascience.com/imbalanced-classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hWQLqhOG_zss",
    "outputId": "23c34f02-184c-48ff-e1b6-17d5d7a07bc5",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    20000\n",
       "4    20000\n",
       "3    20000\n",
       "2    20000\n",
       "1    20000\n",
       "Name: star_rating, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "# separating reviews\n",
    "five_star =  df.loc[ df['star_rating'] == 5]\n",
    "four_star =  df.loc[ df['star_rating'] == 4]\n",
    "three_star =  df.loc[ df['star_rating'] == 3]\n",
    "two_star =  df.loc[ df['star_rating'] == 2]\n",
    "one_star =  df.loc[ df['star_rating'] == 1]\n",
    "\n",
    "# Downsample 5-star class\n",
    "five_star_downsampled = resample(five_star,\n",
    "                                 replace=False,    # sample without replacement\n",
    "                                 n_samples=20000,     # to match minority class\n",
    "                                 random_state=123) # reproducible results\n",
    "\n",
    "# Downsample 4-star class\n",
    "four_star_downsampled = resample(four_star,\n",
    "                                 replace=False,    # sample without replacement\n",
    "                                 n_samples=20000,     # to match minority class\n",
    "                                 random_state=123) # reproducible results\n",
    "\n",
    "# Upsample 3-star class\n",
    "three_star_upsampled = resample(three_star,\n",
    "                                replace=True,     # sample with replacement\n",
    "                                n_samples=20000,    # to match majority class\n",
    "                                random_state=123) # reproducible results\n",
    "\n",
    "# Upsample 2-star class\n",
    "two_star_upsampled = resample(two_star,\n",
    "                              replace=True,     # sample with replacement\n",
    "                              n_samples=20000,    # to match majority class\n",
    "                              random_state=123) # reproducible results\n",
    "\n",
    "# Upsample 1-star class\n",
    "one_star_upsampled = resample(one_star,\n",
    "                              replace=True,     # sample with replacement\n",
    "                              n_samples=20000,    # to match majority class\n",
    "                              random_state=123) # reproducible results\n",
    "\n",
    "balanced_data = pd.concat([five_star_downsampled, four_star_downsampled, three_star_upsampled, two_star_upsampled, one_star_upsampled], axis=0)\n",
    "balanced_data[\"star_rating\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tDByIbuZ-ZgP",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### ii. Test-train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wYP6CT75_zss",
    "outputId": "d9b82143-0983-47e5-846a-fe6d19202941",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (80000,) (80000,) Test:  ((20000,), (20000,))\n"
     ]
    }
   ],
   "source": [
    "# Train - test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "five_star_X_train, five_star_X_test, five_star_Y_train, five_star_Y_test = \\\n",
    "    train_test_split(balanced_data[balanced_data[\"star_rating\"] == 5][\"review_body\"],\n",
    "                     balanced_data[balanced_data[\"star_rating\"] == 5][\"star_rating\"], test_size=0.2, random_state=30)\n",
    "\n",
    "four_star_X_train, four_star_X_test, four_star_Y_train, four_star_Y_test = \\\n",
    "    train_test_split(balanced_data[balanced_data[\"star_rating\"] == 4][\"review_body\"],\n",
    "                     balanced_data[balanced_data[\"star_rating\"] == 4][\"star_rating\"], test_size=0.2, random_state=30)\n",
    "\n",
    "three_star_X_train, three_star_X_test, three_star_Y_train, three_star_Y_test = \\\n",
    "    train_test_split(balanced_data[balanced_data[\"star_rating\"] == 3][\"review_body\"],\n",
    "                     balanced_data[balanced_data[\"star_rating\"] == 3][\"star_rating\"], test_size=0.2, random_state=30)\n",
    "\n",
    "two_star_X_train, two_star_X_test, two_star_Y_train, two_star_Y_test = \\\n",
    "    train_test_split(balanced_data[balanced_data[\"star_rating\"] == 2][\"review_body\"],\n",
    "                     balanced_data[balanced_data[\"star_rating\"] == 2][\"star_rating\"], test_size=0.2, random_state=30)\n",
    "\n",
    "one_star_X_train, one_star_X_test, one_star_Y_train, one_star_Y_test = \\\n",
    "    train_test_split(balanced_data[balanced_data[\"star_rating\"] == 1][\"review_body\"],\n",
    "                     balanced_data[balanced_data[\"star_rating\"] == 1][\"star_rating\"], test_size=0.2, random_state=30)\n",
    "\n",
    "X_train = pd.concat([five_star_X_train, four_star_X_train, three_star_X_train, two_star_X_train, one_star_X_train])\n",
    "X_test = pd.concat([five_star_X_test, four_star_X_test, three_star_X_test, two_star_X_test, one_star_X_test])\n",
    "Y_train = pd.concat([five_star_Y_train, four_star_Y_train, three_star_Y_train, two_star_Y_train, one_star_Y_train])\n",
    "Y_test = pd.concat([five_star_Y_test, four_star_Y_test, three_star_Y_test, two_star_Y_test, one_star_Y_test])\n",
    "\n",
    "print(\"Train: \", X_train.shape, Y_train.shape, \"Test: \", (X_test.shape, Y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0MtAqFyA-x1o",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### iii. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "fKD6IgRS_zst",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import contractions\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def data_preprocessing(data):\n",
    "    # convert all reviews to lower case\n",
    "    data = data.apply(lambda x: \" \".join(x.lower() for x in str(x).split()))\n",
    "\n",
    "    # remove HTML tags as well as URLs from reviews.\n",
    "    data = data.apply(lambda x: BeautifulSoup(x).get_text())\n",
    "    data = data.apply(lambda x: re.sub(r'https?://\\S+|www\\.\\S+', \"\", x))\n",
    "\n",
    "    # contractions\n",
    "    data = data.apply(lambda x:contractions.fix(x))\n",
    "\n",
    "    # remove the non-alpha characters\n",
    "    data = data.apply(lambda x: \" \".join([re.sub(\"[^A-Za-z]+\",\"\", x) for x in nltk.word_tokenize(x)]))\n",
    "\n",
    "    # remove extra spaces among the words\n",
    "    data = data.apply(lambda x: re.sub(' +', ' ', x))\n",
    "\n",
    "    # removing stop words\n",
    "    stop_words=['the', 'a', 'and', 'is', 'be', 'will', 'are']\n",
    "    data = data.apply(lambda x: \" \".join([x for x in x.split() if x not in stop_words]))\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    data = data.apply(lambda x: \" \".join([lemmatizer.lemmatize(w) for w in nltk.word_tokenize(x)]))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "sX4tFZzV_zsu",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train = data_preprocessing(X_train)\n",
    "X_test = data_preprocessing(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YTkIZ5e3_sIk",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2. Word Embedding \n",
    "Reference : https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HRhp7BABFGJA",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### a) Exploring pretrained “word2vec-google-news-300”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "GvkEBY6d_zsu",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Loading 'word2vec-google-news-300' model\n",
    "import gensim.downloader as api\n",
    "wv_google = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TENvIc4aC_h7",
    "outputId": "3218be22-5da1-4036-f89b-6d2b8babe56f",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queen: 0.7118\n"
     ]
    }
   ],
   "source": [
    "# checking semantic similarities\n",
    "# Example 1\n",
    "result = wv_google.most_similar(positive=['woman', 'king'], negative=['man'])\n",
    "print(\"{}: {:.4f}\".format(*result[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cfCwZYiyC-9k",
    "outputId": "a591c6f9-511f-47d3-b1b8-0b07765dfb18",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55674857"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 2\n",
    "wv_google.similarity('excellent', 'outstanding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "aF76GiraC-Ww",
    "outputId": "64f35f60-ce5b-4353-f41d-8f1377070c0a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'car'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 3\n",
    "wv_google.doesnt_match(['fire', 'water', 'land', 'sea', 'air', 'car'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xC53_46iC92o",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### b) Train a Word2Vec model using your own dataset.\n",
    "Reference : https://www.kaggle.com/code/chewzy/tutorial-how-to-train-your-custom-word-embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "YK8IL0xpC8dC",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "full_dataset = pd.concat([X_train, X_test],axis=0)\n",
    "sentences = []\n",
    "for review in full_dataset:\n",
    "  tokens = review.split()\n",
    "  sentences.append(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "zX7sf0OfC8Pi",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "custom_wv_model = Word2Vec(sentences=sentences, size=300, window=11, min_count=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7jXWIydks731",
    "outputId": "c97ff343-7b47-4bed-868d-a54c3f77644d",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avenue: 0.5790\n"
     ]
    }
   ],
   "source": [
    "result = custom_wv_model.most_similar(positive=['woman', 'king'], negative=['man'])\n",
    "print(\"{}: {:.4f}\".format(*result[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QuW7w9jdK96d",
    "outputId": "f210ee19-c476-4e51-9f18-a514c37dd60a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7977613"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 1\n",
    "custom_wv_model.similarity('excellent', 'outstanding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yL2B9EdVK-Le",
    "outputId": "771ad58c-29f9-44e7-c643-06cad0b5e48b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('decent', 0.8035435676574707),\n",
       " ('great', 0.7997811436653137),\n",
       " ('nice', 0.6498663425445557),\n",
       " ('high', 0.6413455009460449),\n",
       " ('excellent', 0.6235317587852478),\n",
       " ('ok', 0.6038389205932617),\n",
       " ('fantastic', 0.5990501046180725),\n",
       " ('poor', 0.5846014022827148),\n",
       " ('bad', 0.5676741003990173),\n",
       " ('control', 0.5650346875190735)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 2\n",
    "custom_wv_model.most_similar(\"good\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1oZol_elYdTY",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Question : What do you conclude from comparing vectors generated by yourself and the pretrained model? Which of the Word2Vec models seems to encode semantic similarities between words better?\n",
    "Answer : Pretrained Google word2vec model have diverse variety of words in its vocabulary and therefore is able to capture semantic similarities of diverse set of  words better. For our own custom build model we need to a large and diverse corpus to train to get the desired results.\n",
    "Hence, Pretrained google word2vec is better than our own custom build."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OTZbkosEK-oL",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3. Simple models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cnZ5QBXHOyRQ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Average Word2Vec vectors for each review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "odP5Z9Su_zsu",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# average word2vec\n",
    "def get_avg_wor2vec(_reviews):\n",
    "    word_list = _reviews.split()\n",
    "    words_cnt = 0\n",
    "    word_vector = np.zeros(300)\n",
    "    for word in word_list:\n",
    "        if word in wv_google:\n",
    "            word_vector += wv_google[word]\n",
    "            words_cnt += 1\n",
    "    if words_cnt != 0:\n",
    "        word_vector /= words_cnt\n",
    "    return word_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "OdJX8kEk_zsv",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_vec = []\n",
    "for reviews in X_train:\n",
    "    train_vec.append(get_avg_wor2vec(reviews))\n",
    "train_vec = np.array(train_vec)\n",
    "\n",
    "test_vec = []\n",
    "for reviews in X_test:\n",
    "    test_vec.append(get_avg_wor2vec(reviews))\n",
    "test_vec = np.array(test_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c4nbXiShPF50",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### a) Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "g2B22kiyPFlM",
    "outputId": "ad12a3e6-80a0-46ea-dc93-5bb5a10e9a59",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-a57d2a3f-433b-42ac-9cd2-9812ab026d85\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macro avg</th>\n",
       "      <th>weighted avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.469344</td>\n",
       "      <td>0.220267</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.397626</td>\n",
       "      <td>0.697067</td>\n",
       "      <td>0.33365</td>\n",
       "      <td>0.429588</td>\n",
       "      <td>0.429588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.549250</td>\n",
       "      <td>0.704250</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>0.380250</td>\n",
       "      <td>0.33365</td>\n",
       "      <td>0.333650</td>\n",
       "      <td>0.333650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1-score</th>\n",
       "      <td>0.506163</td>\n",
       "      <td>0.335577</td>\n",
       "      <td>0.001995</td>\n",
       "      <td>0.061794</td>\n",
       "      <td>0.492074</td>\n",
       "      <td>0.33365</td>\n",
       "      <td>0.279520</td>\n",
       "      <td>0.279520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>support</th>\n",
       "      <td>4000.000000</td>\n",
       "      <td>4000.000000</td>\n",
       "      <td>4000.000000</td>\n",
       "      <td>4000.000000</td>\n",
       "      <td>4000.000000</td>\n",
       "      <td>0.33365</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a57d2a3f-433b-42ac-9cd2-9812ab026d85')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-a57d2a3f-433b-42ac-9cd2-9812ab026d85 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-a57d2a3f-433b-42ac-9cd2-9812ab026d85');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                     1            2  ...     macro avg  weighted avg\n",
       "precision     0.469344     0.220267  ...      0.429588      0.429588\n",
       "recall        0.549250     0.704250  ...      0.333650      0.333650\n",
       "f1-score      0.506163     0.335577  ...      0.279520      0.279520\n",
       "support    4000.000000  4000.000000  ...  20000.000000  20000.000000\n",
       "\n",
       "[4 rows x 8 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import classification_report\n",
    "perceptron = Perceptron(max_iter=1000, random_state=0)\n",
    "perceptron.fit(train_vec,Y_train)\n",
    "y_test_predicted = perceptron.predict(test_vec)\n",
    "\n",
    "report = classification_report(Y_test, y_test_predicted, output_dict=True)\n",
    "pd.DataFrame.from_dict(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f7qM5wZhPFAP",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### b) SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "cZZjzzlwPEqR",
    "outputId": "65ed7b9e-0e40-4f7b-a4e9-529da10969c4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-0f390216-7c0a-45fd-b381-5d61f6a6a254\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macro avg</th>\n",
       "      <th>weighted avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.511508</td>\n",
       "      <td>0.399172</td>\n",
       "      <td>0.401657</td>\n",
       "      <td>0.435879</td>\n",
       "      <td>0.585394</td>\n",
       "      <td>0.4851</td>\n",
       "      <td>0.466722</td>\n",
       "      <td>0.466722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.716750</td>\n",
       "      <td>0.265250</td>\n",
       "      <td>0.375750</td>\n",
       "      <td>0.298250</td>\n",
       "      <td>0.769500</td>\n",
       "      <td>0.4851</td>\n",
       "      <td>0.485100</td>\n",
       "      <td>0.485100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1-score</th>\n",
       "      <td>0.596981</td>\n",
       "      <td>0.318714</td>\n",
       "      <td>0.388272</td>\n",
       "      <td>0.354164</td>\n",
       "      <td>0.664938</td>\n",
       "      <td>0.4851</td>\n",
       "      <td>0.464614</td>\n",
       "      <td>0.464614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>support</th>\n",
       "      <td>4000.000000</td>\n",
       "      <td>4000.000000</td>\n",
       "      <td>4000.000000</td>\n",
       "      <td>4000.000000</td>\n",
       "      <td>4000.000000</td>\n",
       "      <td>0.4851</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0f390216-7c0a-45fd-b381-5d61f6a6a254')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-0f390216-7c0a-45fd-b381-5d61f6a6a254 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-0f390216-7c0a-45fd-b381-5d61f6a6a254');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                     1            2  ...     macro avg  weighted avg\n",
       "precision     0.511508     0.399172  ...      0.466722      0.466722\n",
       "recall        0.716750     0.265250  ...      0.485100      0.485100\n",
       "f1-score      0.596981     0.318714  ...      0.464614      0.464614\n",
       "support    4000.000000  4000.000000  ...  20000.000000  20000.000000\n",
       "\n",
       "[4 rows x 8 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "svm = LinearSVC(multi_class=\"ovr\", random_state=0)\n",
    "svm.fit(train_vec,Y_train)\n",
    "y_test_predicted = svm.predict(test_vec)\n",
    "\n",
    "report = classification_report(Y_test, y_test_predicted, output_dict=True)\n",
    "pd.DataFrame.from_dict(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IaIFfvO6SOpH",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Comparing performance of Perceptron & SVM model trained using TF-IDF and Word2Vec features\n",
    "Reading accuracy values of Perceptron and SVM model from HW1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "y3vyxY_LSPOt",
    "outputId": "eac1262b-04c1-42a4-8f94-4d2acd5047db",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-464ccf3a-103b-47e2-89ca-db79ae5fe926\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macro avg</th>\n",
       "      <th>weighted avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>precision</td>\n",
       "      <td>0.529361</td>\n",
       "      <td>0.302142</td>\n",
       "      <td>0.320768</td>\n",
       "      <td>0.383514</td>\n",
       "      <td>0.590234</td>\n",
       "      <td>0.414</td>\n",
       "      <td>0.425204</td>\n",
       "      <td>0.425204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>recall</td>\n",
       "      <td>0.462000</td>\n",
       "      <td>0.469000</td>\n",
       "      <td>0.221250</td>\n",
       "      <td>0.361750</td>\n",
       "      <td>0.556000</td>\n",
       "      <td>0.414</td>\n",
       "      <td>0.414000</td>\n",
       "      <td>0.414000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f1-score</td>\n",
       "      <td>0.493392</td>\n",
       "      <td>0.367519</td>\n",
       "      <td>0.261873</td>\n",
       "      <td>0.372314</td>\n",
       "      <td>0.572606</td>\n",
       "      <td>0.414</td>\n",
       "      <td>0.413541</td>\n",
       "      <td>0.413541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>support</td>\n",
       "      <td>4000.000000</td>\n",
       "      <td>4000.000000</td>\n",
       "      <td>4000.000000</td>\n",
       "      <td>4000.000000</td>\n",
       "      <td>4000.000000</td>\n",
       "      <td>0.414</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-464ccf3a-103b-47e2-89ca-db79ae5fe926')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-464ccf3a-103b-47e2-89ca-db79ae5fe926 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-464ccf3a-103b-47e2-89ca-db79ae5fe926');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "  Unnamed: 0            1            2  ...  accuracy     macro avg  weighted avg\n",
       "0  precision     0.529361     0.302142  ...     0.414      0.425204      0.425204\n",
       "1     recall     0.462000     0.469000  ...     0.414      0.414000      0.414000\n",
       "2   f1-score     0.493392     0.367519  ...     0.414      0.413541      0.413541\n",
       "3    support  4000.000000  4000.000000  ...     0.414  20000.000000  20000.000000\n",
       "\n",
       "[4 rows x 9 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perceptron_using_tfidf = pd.read_csv(\"perceptron.csv\")\n",
    "perceptron_using_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "gnKoj4JDSPjs",
    "outputId": "22c9ef7f-5d19-4e70-fe3e-b8c41227963e",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-9f07d688-de1a-4d51-89db-2289474899ab\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macro avg</th>\n",
       "      <th>weighted avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>precision</td>\n",
       "      <td>0.563424</td>\n",
       "      <td>0.404890</td>\n",
       "      <td>0.424075</td>\n",
       "      <td>0.472624</td>\n",
       "      <td>0.639847</td>\n",
       "      <td>0.51355</td>\n",
       "      <td>0.500972</td>\n",
       "      <td>0.500972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>recall</td>\n",
       "      <td>0.676250</td>\n",
       "      <td>0.339500</td>\n",
       "      <td>0.384000</td>\n",
       "      <td>0.416500</td>\n",
       "      <td>0.751500</td>\n",
       "      <td>0.51355</td>\n",
       "      <td>0.513550</td>\n",
       "      <td>0.513550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f1-score</td>\n",
       "      <td>0.614703</td>\n",
       "      <td>0.369323</td>\n",
       "      <td>0.403044</td>\n",
       "      <td>0.442791</td>\n",
       "      <td>0.691193</td>\n",
       "      <td>0.51355</td>\n",
       "      <td>0.504211</td>\n",
       "      <td>0.504211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>support</td>\n",
       "      <td>4000.000000</td>\n",
       "      <td>4000.000000</td>\n",
       "      <td>4000.000000</td>\n",
       "      <td>4000.000000</td>\n",
       "      <td>4000.000000</td>\n",
       "      <td>0.51355</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9f07d688-de1a-4d51-89db-2289474899ab')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-9f07d688-de1a-4d51-89db-2289474899ab button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-9f07d688-de1a-4d51-89db-2289474899ab');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "  Unnamed: 0            1            2  ...  accuracy     macro avg  weighted avg\n",
       "0  precision     0.563424     0.404890  ...   0.51355      0.500972      0.500972\n",
       "1     recall     0.676250     0.339500  ...   0.51355      0.513550      0.513550\n",
       "2   f1-score     0.614703     0.369323  ...   0.51355      0.504211      0.504211\n",
       "3    support  4000.000000  4000.000000  ...   0.51355  20000.000000  20000.000000\n",
       "\n",
       "[4 rows x 9 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_using_tfidf = pd.read_csv(\"svm.csv\")\n",
    "svm_using_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1BQ2PlnbSQND",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Question : What do you conclude from comparing performances for the models trained using the two different feature types (TF-IDF and your trained Word2Vec features)?\n",
    "Answer : Simple model (Perceptron & SVM) trained using TF-IDF has better accuracy as compared to model trained using Word2Vec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J9Y2ubdKSQd-",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 4. Feedforward Neural Networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hSVjbjOCvdGr",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### a) Train using average Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "rYS--WHa_zsv",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-K16kPsw_zsv",
    "outputId": "2f1832c2-d9f2-4670-d8e8-2bd1ae310be8",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 50)                15050     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                510       \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 10)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 55        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,615\n",
      "Trainable params: 15,615\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(50, input_shape = (300,),activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "sgd = SGD(0.01)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=sgd, metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "rGJ1Efe_vE7U",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train_vec = X_train.apply(lambda x: get_avg_wor2vec(x)).to_numpy()\n",
    "X_test_vec = X_test.apply(lambda x: get_avg_wor2vec(x)).to_numpy()\n",
    "X_train_vec = np.concatenate([np.concatenate(X_train_vec, axis=0)], axis=0).reshape(-1, 300)\n",
    "X_test_vec = np.concatenate([np.concatenate(X_test_vec, axis=0)], axis=0).reshape(-1, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "qQhqdoYXrJ-H",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Y_train_np = Y_train.apply(lambda x : x - 1)\n",
    "Y_train_np = Y_train_np.to_numpy()\n",
    "Y_test_np = Y_test.apply(lambda x : x - 1)\n",
    "Y_test_np = Y_test_np.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pC8Qf8093qyu",
    "outputId": "050d45ff-33db-4dd3-9097-d26bd5ca716e",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.5815 - accuracy: 0.2971\n",
      "Epoch 2/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.4592 - accuracy: 0.3608\n",
      "Epoch 3/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.3608 - accuracy: 0.3911\n",
      "Epoch 4/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.3119 - accuracy: 0.4102\n",
      "Epoch 5/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.2834 - accuracy: 0.4262\n",
      "Epoch 6/100\n",
      "2500/2500 [==============================] - 4s 2ms/step - loss: 1.2645 - accuracy: 0.4386\n",
      "Epoch 7/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.2511 - accuracy: 0.4466\n",
      "Epoch 8/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.2426 - accuracy: 0.4523\n",
      "Epoch 9/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.2345 - accuracy: 0.4576\n",
      "Epoch 10/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.2268 - accuracy: 0.4587\n",
      "Epoch 11/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.2214 - accuracy: 0.4635\n",
      "Epoch 12/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.2169 - accuracy: 0.4652\n",
      "Epoch 13/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.2132 - accuracy: 0.4655\n",
      "Epoch 14/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.2098 - accuracy: 0.4675\n",
      "Epoch 15/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.2063 - accuracy: 0.4706\n",
      "Epoch 16/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.2042 - accuracy: 0.4719\n",
      "Epoch 17/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.2005 - accuracy: 0.4729\n",
      "Epoch 18/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1992 - accuracy: 0.4741\n",
      "Epoch 19/100\n",
      "2500/2500 [==============================] - 4s 2ms/step - loss: 1.1988 - accuracy: 0.4735\n",
      "Epoch 20/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1962 - accuracy: 0.4730\n",
      "Epoch 21/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1920 - accuracy: 0.4764\n",
      "Epoch 22/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1932 - accuracy: 0.4763\n",
      "Epoch 23/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1902 - accuracy: 0.4774\n",
      "Epoch 24/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1878 - accuracy: 0.4784\n",
      "Epoch 25/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1857 - accuracy: 0.4779\n",
      "Epoch 26/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1846 - accuracy: 0.4804\n",
      "Epoch 27/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1823 - accuracy: 0.4799\n",
      "Epoch 28/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1838 - accuracy: 0.4785\n",
      "Epoch 29/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1807 - accuracy: 0.4820\n",
      "Epoch 30/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1786 - accuracy: 0.4829\n",
      "Epoch 31/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1785 - accuracy: 0.4818\n",
      "Epoch 32/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1770 - accuracy: 0.4818\n",
      "Epoch 33/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1763 - accuracy: 0.4808\n",
      "Epoch 34/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1745 - accuracy: 0.4807\n",
      "Epoch 35/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1736 - accuracy: 0.4824\n",
      "Epoch 36/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1718 - accuracy: 0.4828\n",
      "Epoch 37/100\n",
      "2500/2500 [==============================] - 4s 2ms/step - loss: 1.1723 - accuracy: 0.4843\n",
      "Epoch 38/100\n",
      "2500/2500 [==============================] - 4s 2ms/step - loss: 1.1703 - accuracy: 0.4850\n",
      "Epoch 39/100\n",
      "2500/2500 [==============================] - 4s 2ms/step - loss: 1.1690 - accuracy: 0.4867\n",
      "Epoch 40/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1675 - accuracy: 0.4855\n",
      "Epoch 41/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1664 - accuracy: 0.4882\n",
      "Epoch 42/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1659 - accuracy: 0.4856\n",
      "Epoch 43/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1661 - accuracy: 0.4877\n",
      "Epoch 44/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1636 - accuracy: 0.4851\n",
      "Epoch 45/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1627 - accuracy: 0.4884\n",
      "Epoch 46/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1611 - accuracy: 0.4862\n",
      "Epoch 47/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1617 - accuracy: 0.4879\n",
      "Epoch 48/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1596 - accuracy: 0.4893\n",
      "Epoch 49/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1576 - accuracy: 0.4875\n",
      "Epoch 50/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1578 - accuracy: 0.4899\n",
      "Epoch 51/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1572 - accuracy: 0.4890\n",
      "Epoch 52/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1564 - accuracy: 0.4883\n",
      "Epoch 53/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1538 - accuracy: 0.4917\n",
      "Epoch 54/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1537 - accuracy: 0.4898\n",
      "Epoch 55/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1529 - accuracy: 0.4897\n",
      "Epoch 56/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1542 - accuracy: 0.4895\n",
      "Epoch 57/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1516 - accuracy: 0.4909\n",
      "Epoch 58/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1514 - accuracy: 0.4918\n",
      "Epoch 59/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1518 - accuracy: 0.4904\n",
      "Epoch 60/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1499 - accuracy: 0.4940\n",
      "Epoch 61/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1489 - accuracy: 0.4934\n",
      "Epoch 62/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1482 - accuracy: 0.4923\n",
      "Epoch 63/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1463 - accuracy: 0.4928\n",
      "Epoch 64/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1461 - accuracy: 0.4942\n",
      "Epoch 65/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1454 - accuracy: 0.4928\n",
      "Epoch 66/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1451 - accuracy: 0.4935\n",
      "Epoch 67/100\n",
      "2500/2500 [==============================] - 4s 2ms/step - loss: 1.1442 - accuracy: 0.4943\n",
      "Epoch 68/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1434 - accuracy: 0.4945\n",
      "Epoch 69/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1439 - accuracy: 0.4941\n",
      "Epoch 70/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1429 - accuracy: 0.4939\n",
      "Epoch 71/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1401 - accuracy: 0.4968\n",
      "Epoch 72/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1397 - accuracy: 0.4959\n",
      "Epoch 73/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1383 - accuracy: 0.4962\n",
      "Epoch 74/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1402 - accuracy: 0.4961\n",
      "Epoch 75/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1396 - accuracy: 0.4972\n",
      "Epoch 76/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1383 - accuracy: 0.4953\n",
      "Epoch 77/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1363 - accuracy: 0.4986\n",
      "Epoch 78/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1359 - accuracy: 0.4949\n",
      "Epoch 79/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1363 - accuracy: 0.4938\n",
      "Epoch 80/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1355 - accuracy: 0.4975\n",
      "Epoch 81/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1338 - accuracy: 0.4990\n",
      "Epoch 82/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1336 - accuracy: 0.4979\n",
      "Epoch 83/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1335 - accuracy: 0.4970\n",
      "Epoch 84/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1334 - accuracy: 0.4991\n",
      "Epoch 85/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1314 - accuracy: 0.4981\n",
      "Epoch 86/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1331 - accuracy: 0.4969\n",
      "Epoch 87/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1301 - accuracy: 0.5007\n",
      "Epoch 88/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1314 - accuracy: 0.4996\n",
      "Epoch 89/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1277 - accuracy: 0.4992\n",
      "Epoch 90/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1298 - accuracy: 0.4981\n",
      "Epoch 91/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1295 - accuracy: 0.4990\n",
      "Epoch 92/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1267 - accuracy: 0.5011\n",
      "Epoch 93/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1279 - accuracy: 0.5005\n",
      "Epoch 94/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1268 - accuracy: 0.5013\n",
      "Epoch 95/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1279 - accuracy: 0.4993\n",
      "Epoch 96/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1254 - accuracy: 0.5006\n",
      "Epoch 97/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1243 - accuracy: 0.5016\n",
      "Epoch 98/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1239 - accuracy: 0.5017\n",
      "Epoch 99/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1243 - accuracy: 0.5005\n",
      "Epoch 100/100\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 1.1237 - accuracy: 0.5005\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7ab2d54f50>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_vec, Y_train_np, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HXmTJKsIbtk-",
    "outputId": "d728204f-22fd-4d45-a842-fdcdc4f3b059",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 1s 1ms/step - loss: 1.1291 - accuracy: 0.5060\n",
      "Test Loss: 1.129124641418457\n",
      "Test Accuracy: 0.5059999823570251\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test_vec, Y_test_np)\n",
    "\n",
    "print('Test Loss:', test_loss)\n",
    "print('Test Accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wSwkkoiUqKpe",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### b) Generate the input feature by concatenating the first 10 Word2Vec vectors for each review as the input feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "zd-QZ0En6ro6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_concatenated_first10_feature_vector(dataset):\n",
    "  feature_10_word2vec = []\n",
    "  for reviews in dataset:\n",
    "    words = reviews.split()\n",
    "    max_words = 10\n",
    "    review_embedding = []\n",
    "    for word in words:\n",
    "      if len(review_embedding) < max_words:\n",
    "        word_vec = np.zeros(300)\n",
    "        if word in wv_google:\n",
    "          word_vec += wv_google[word]\n",
    "        review_embedding.append(word_vec)\n",
    "    if len(review_embedding) < max_words:\n",
    "      while len(review_embedding) != max_words:\n",
    "        review_embedding.append(np.zeros(300))\n",
    "    review_embedding = np.concatenate(review_embedding)\n",
    "    feature_10_word2vec.append(review_embedding)\n",
    "  feature_10_word2vec = np.array(feature_10_word2vec)\n",
    "  return feature_10_word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6FPeUX-9qhET",
    "outputId": "d5eb561b-fed9-42d1-c1b9-956617717402",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 50)                150050    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 10)                510       \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 10)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 5)                 55        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 150,615\n",
      "Trainable params: 150,615\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(50, input_shape = (3000,),activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "sgd = SGD(0.01)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=sgd, metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "HVzQMYSW7NHQ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train_10_word2vec = get_concatenated_first10_feature_vector(X_train)\n",
    "X_test_10_word2vec = get_concatenated_first10_feature_vector(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mfs9WL9iqh_c",
    "outputId": "9021e440-f48f-429e-f48f-a2f9cc3d3369",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 1.5196 - accuracy: 0.2954\n",
      "Epoch 2/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 1.3897 - accuracy: 0.3737\n",
      "Epoch 3/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 1.3467 - accuracy: 0.4001\n",
      "Epoch 4/100\n",
      "2500/2500 [==============================] - 6s 2ms/step - loss: 1.3251 - accuracy: 0.4128\n",
      "Epoch 5/100\n",
      "2500/2500 [==============================] - 6s 2ms/step - loss: 1.3092 - accuracy: 0.4217\n",
      "Epoch 6/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 1.2962 - accuracy: 0.4297\n",
      "Epoch 7/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 1.2845 - accuracy: 0.4346\n",
      "Epoch 8/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 1.2743 - accuracy: 0.4410\n",
      "Epoch 9/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 1.2620 - accuracy: 0.4441\n",
      "Epoch 10/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 1.2529 - accuracy: 0.4513\n",
      "Epoch 11/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 1.2432 - accuracy: 0.4537\n",
      "Epoch 12/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 1.2347 - accuracy: 0.4599\n",
      "Epoch 13/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 1.2225 - accuracy: 0.4679\n",
      "Epoch 14/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 1.2101 - accuracy: 0.4720\n",
      "Epoch 15/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 1.2001 - accuracy: 0.4759\n",
      "Epoch 16/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 1.1882 - accuracy: 0.4828\n",
      "Epoch 17/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 1.1756 - accuracy: 0.4897\n",
      "Epoch 18/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 1.1647 - accuracy: 0.4930\n",
      "Epoch 19/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 1.1518 - accuracy: 0.4995\n",
      "Epoch 20/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 1.1395 - accuracy: 0.5057\n",
      "Epoch 21/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 1.1255 - accuracy: 0.5117\n",
      "Epoch 22/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 1.1129 - accuracy: 0.5162\n",
      "Epoch 23/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 1.0989 - accuracy: 0.5218\n",
      "Epoch 24/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 1.0865 - accuracy: 0.5291\n",
      "Epoch 25/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 1.0729 - accuracy: 0.5355\n",
      "Epoch 26/100\n",
      "2500/2500 [==============================] - 6s 2ms/step - loss: 1.0574 - accuracy: 0.5416\n",
      "Epoch 27/100\n",
      "2500/2500 [==============================] - 6s 2ms/step - loss: 1.0434 - accuracy: 0.5459\n",
      "Epoch 28/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 1.0294 - accuracy: 0.5529\n",
      "Epoch 29/100\n",
      "2500/2500 [==============================] - 6s 2ms/step - loss: 1.0151 - accuracy: 0.5593\n",
      "Epoch 30/100\n",
      "2500/2500 [==============================] - 6s 2ms/step - loss: 1.0029 - accuracy: 0.5636\n",
      "Epoch 31/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.9907 - accuracy: 0.5718\n",
      "Epoch 32/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.9771 - accuracy: 0.5756\n",
      "Epoch 33/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.9637 - accuracy: 0.5794\n",
      "Epoch 34/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.9494 - accuracy: 0.5861\n",
      "Epoch 35/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.9372 - accuracy: 0.5915\n",
      "Epoch 36/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.9253 - accuracy: 0.5958\n",
      "Epoch 37/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.9127 - accuracy: 0.6021\n",
      "Epoch 38/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.9017 - accuracy: 0.6076\n",
      "Epoch 39/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.8903 - accuracy: 0.6136\n",
      "Epoch 40/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.8746 - accuracy: 0.6175\n",
      "Epoch 41/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.8676 - accuracy: 0.6229\n",
      "Epoch 42/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.8551 - accuracy: 0.6273\n",
      "Epoch 43/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.8407 - accuracy: 0.6319\n",
      "Epoch 44/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.8361 - accuracy: 0.6345\n",
      "Epoch 45/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.8215 - accuracy: 0.6424\n",
      "Epoch 46/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.8114 - accuracy: 0.6469\n",
      "Epoch 47/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.7997 - accuracy: 0.6517\n",
      "Epoch 48/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.7932 - accuracy: 0.6544\n",
      "Epoch 49/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.7869 - accuracy: 0.6567\n",
      "Epoch 50/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.7791 - accuracy: 0.6583\n",
      "Epoch 51/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.7669 - accuracy: 0.6663\n",
      "Epoch 52/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.7585 - accuracy: 0.6684\n",
      "Epoch 53/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.7488 - accuracy: 0.6715\n",
      "Epoch 54/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.7426 - accuracy: 0.6745\n",
      "Epoch 55/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.7315 - accuracy: 0.6807\n",
      "Epoch 56/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.7270 - accuracy: 0.6825\n",
      "Epoch 57/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.7207 - accuracy: 0.6859\n",
      "Epoch 58/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.7110 - accuracy: 0.6899\n",
      "Epoch 59/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.7042 - accuracy: 0.6912\n",
      "Epoch 60/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.6973 - accuracy: 0.6959\n",
      "Epoch 61/100\n",
      "2500/2500 [==============================] - 6s 2ms/step - loss: 0.6890 - accuracy: 0.6967\n",
      "Epoch 62/100\n",
      "2500/2500 [==============================] - 6s 2ms/step - loss: 0.6842 - accuracy: 0.6993\n",
      "Epoch 63/100\n",
      "2500/2500 [==============================] - 6s 2ms/step - loss: 0.6781 - accuracy: 0.7028\n",
      "Epoch 64/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.6689 - accuracy: 0.7082\n",
      "Epoch 65/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.6662 - accuracy: 0.7085\n",
      "Epoch 66/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.6600 - accuracy: 0.7106\n",
      "Epoch 67/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.6497 - accuracy: 0.7155\n",
      "Epoch 68/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.6491 - accuracy: 0.7146\n",
      "Epoch 69/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.6429 - accuracy: 0.7186\n",
      "Epoch 70/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.6352 - accuracy: 0.7209\n",
      "Epoch 71/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.6289 - accuracy: 0.7239\n",
      "Epoch 72/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.6241 - accuracy: 0.7262\n",
      "Epoch 73/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.6199 - accuracy: 0.7300\n",
      "Epoch 74/100\n",
      "2500/2500 [==============================] - 6s 2ms/step - loss: 0.6135 - accuracy: 0.7304\n",
      "Epoch 75/100\n",
      "2500/2500 [==============================] - 6s 2ms/step - loss: 0.6074 - accuracy: 0.7343\n",
      "Epoch 76/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.6060 - accuracy: 0.7341\n",
      "Epoch 77/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.6006 - accuracy: 0.7355\n",
      "Epoch 78/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.5972 - accuracy: 0.7378\n",
      "Epoch 79/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.5900 - accuracy: 0.7423\n",
      "Epoch 80/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.5855 - accuracy: 0.7417\n",
      "Epoch 81/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.5824 - accuracy: 0.7446\n",
      "Epoch 82/100\n",
      "2500/2500 [==============================] - 6s 2ms/step - loss: 0.5760 - accuracy: 0.7484\n",
      "Epoch 83/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.5710 - accuracy: 0.7518\n",
      "Epoch 84/100\n",
      "2500/2500 [==============================] - 6s 2ms/step - loss: 0.5691 - accuracy: 0.7511\n",
      "Epoch 85/100\n",
      "2500/2500 [==============================] - 6s 2ms/step - loss: 0.5657 - accuracy: 0.7518\n",
      "Epoch 86/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.5601 - accuracy: 0.7558\n",
      "Epoch 87/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.5561 - accuracy: 0.7576\n",
      "Epoch 88/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.5524 - accuracy: 0.7576\n",
      "Epoch 89/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.5508 - accuracy: 0.7578\n",
      "Epoch 90/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.5446 - accuracy: 0.7627\n",
      "Epoch 91/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.5386 - accuracy: 0.7642\n",
      "Epoch 92/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.5334 - accuracy: 0.7657\n",
      "Epoch 93/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.5340 - accuracy: 0.7661\n",
      "Epoch 94/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.5331 - accuracy: 0.7683\n",
      "Epoch 95/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.5277 - accuracy: 0.7703\n",
      "Epoch 96/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.5254 - accuracy: 0.7710\n",
      "Epoch 97/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.5235 - accuracy: 0.7717\n",
      "Epoch 98/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.5250 - accuracy: 0.7696\n",
      "Epoch 99/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.5149 - accuracy: 0.7756\n",
      "Epoch 100/100\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.5087 - accuracy: 0.7777\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7ab25b4ed0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_10_word2vec, Y_train_np, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u09qHBJ_bflu",
    "outputId": "6eb7b7ed-ce2d-4629-8ff1-574d7dfe418b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 1s 2ms/step - loss: 3.7096 - accuracy: 0.4054\n",
      "Test Loss: 3.709632396697998\n",
      "Test Accuracy: 0.4054499864578247\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test_10_word2vec, Y_test_np)\n",
    "\n",
    "print('Test Loss:', test_loss)\n",
    "print('Test Accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZntjbfbPeEZm",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Question : What do you conclude by comparing accuracy values you obtain with those obtained in the “’Simple Models” section?\n",
    "Answer : As compared to simple model (Perceptron test accuracy = 33.365% & SVM test accuacy = 48.51%) first version of FNN (trained on complete word2vec, test accuracy = 50.60%) performed better than the simple model.\n",
    "\n",
    "Where as the second version of FNN (10 word2vec concatenated, test accuracy = 40.54%) performed better than the perceptron however Simple model SVM accuracy (SVM test accuacy = 48.51%) is better in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XlpDxJAg_zsw",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 5. Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z5DSGmmuwOnv",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### a) Simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "vXQkEO_-9Gp4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_first20_feature_embedding(dataset):\n",
    "  feature_vec_embedding = []\n",
    "  for reviews in dataset:\n",
    "    words = reviews.split()\n",
    "    max_vocab = 20\n",
    "    review_embedding = []\n",
    "    for word in words:\n",
    "      if len(review_embedding) < max_vocab:\n",
    "        word_embedd = np.zeros(300)\n",
    "        if word in wv_google:\n",
    "          word_embedd += wv_google[word]\n",
    "          review_embedding.append(word_embedd)\n",
    "      else:\n",
    "        break\n",
    "    if len(review_embedding) < max_vocab:\n",
    "      while len(review_embedding) != max_vocab:\n",
    "        review_embedding.append(np.zeros(300))\n",
    "    feature_vec_embedding.append(review_embedding)\n",
    "  feature_vec_embedding = np.array(feature_vec_embedding)\n",
    "  return feature_vec_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "QLT_lC2o9IIo",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train_vec_embedding = get_first20_feature_embedding(X_train)\n",
    "X_test_vec_embedding = get_first20_feature_embedding(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tZYTy6Lx_zsw",
    "outputId": "a898fb8e-68ef-4771-e8c8-ea593e6e80b2",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn (SimpleRNN)      (None, 20)                6420      \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 5)                 105       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,525\n",
      "Trainable params: 6,525\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# building RNN model\n",
    "from keras.layers import SimpleRNN\n",
    "model = keras.Sequential()\n",
    "model.add(SimpleRNN(20, activation='relu'))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "sgd = SGD(0.001)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=sgd, metrics=[\"accuracy\"])\n",
    "model.build(input_shape=(None, 20, 300))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DZKBTvMzJUUe",
    "outputId": "f7ab632d-c290-4b5f-8a8d-caf736b6a5ad",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2500/2500 [==============================] - 12s 4ms/step - loss: 1.6167 - accuracy: 0.2150\n",
      "Epoch 2/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 1.6135 - accuracy: 0.2167\n",
      "Epoch 3/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 1.6124 - accuracy: 0.2160\n",
      "Epoch 4/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 1.6114 - accuracy: 0.2178\n",
      "Epoch 5/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 1.6104 - accuracy: 0.2190\n",
      "Epoch 6/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.6095 - accuracy: 0.2200\n",
      "Epoch 7/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.6084 - accuracy: 0.2214\n",
      "Epoch 8/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.6071 - accuracy: 0.2234\n",
      "Epoch 9/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.6048 - accuracy: 0.2284\n",
      "Epoch 10/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.5978 - accuracy: 0.2396\n",
      "Epoch 11/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.5581 - accuracy: 0.2785\n",
      "Epoch 12/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.4989 - accuracy: 0.3213\n",
      "Epoch 13/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.4469 - accuracy: 0.3476\n",
      "Epoch 14/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.4062 - accuracy: 0.3637\n",
      "Epoch 15/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.3728 - accuracy: 0.3735\n",
      "Epoch 16/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.3510 - accuracy: 0.3809\n",
      "Epoch 17/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.3373 - accuracy: 0.3880\n",
      "Epoch 18/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.3282 - accuracy: 0.3917\n",
      "Epoch 19/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 1.3211 - accuracy: 0.3953\n",
      "Epoch 20/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.3154 - accuracy: 0.3990\n",
      "Epoch 21/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.3105 - accuracy: 0.4011\n",
      "Epoch 22/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.3067 - accuracy: 0.4038\n",
      "Epoch 23/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.3025 - accuracy: 0.4053\n",
      "Epoch 24/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.2987 - accuracy: 0.4075\n",
      "Epoch 25/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.2954 - accuracy: 0.4103\n",
      "Epoch 26/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.2918 - accuracy: 0.4109\n",
      "Epoch 27/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.2888 - accuracy: 0.4124\n",
      "Epoch 28/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.2857 - accuracy: 0.4159\n",
      "Epoch 29/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.2828 - accuracy: 0.4161\n",
      "Epoch 30/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.2793 - accuracy: 0.4208\n",
      "Epoch 31/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.2765 - accuracy: 0.4226\n",
      "Epoch 32/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.2740 - accuracy: 0.4228\n",
      "Epoch 33/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.2709 - accuracy: 0.4250\n",
      "Epoch 34/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 1.2661 - accuracy: 0.4301\n",
      "Epoch 35/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 1.2620 - accuracy: 0.4322\n",
      "Epoch 36/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.2579 - accuracy: 0.4352\n",
      "Epoch 37/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.2540 - accuracy: 0.4368\n",
      "Epoch 38/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.2505 - accuracy: 0.4401\n",
      "Epoch 39/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 1.2469 - accuracy: 0.4421\n",
      "Epoch 40/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.2438 - accuracy: 0.4446\n",
      "Epoch 41/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 1.2404 - accuracy: 0.4472\n",
      "Epoch 42/100\n",
      "2500/2500 [==============================] - 15s 6ms/step - loss: 1.2372 - accuracy: 0.4470\n",
      "Epoch 43/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 1.2345 - accuracy: 0.4515\n",
      "Epoch 44/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.2321 - accuracy: 0.4528\n",
      "Epoch 45/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.2294 - accuracy: 0.4538\n",
      "Epoch 46/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.2259 - accuracy: 0.4562\n",
      "Epoch 47/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 1.2246 - accuracy: 0.4558\n",
      "Epoch 48/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 1.2219 - accuracy: 0.4594\n",
      "Epoch 49/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.2196 - accuracy: 0.4590\n",
      "Epoch 50/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 1.2177 - accuracy: 0.4606\n",
      "Epoch 51/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.2159 - accuracy: 0.4602\n",
      "Epoch 52/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 1.2139 - accuracy: 0.4618\n",
      "Epoch 53/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.2117 - accuracy: 0.4637\n",
      "Epoch 54/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.2103 - accuracy: 0.4644\n",
      "Epoch 55/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.2090 - accuracy: 0.4645\n",
      "Epoch 56/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.2075 - accuracy: 0.4658\n",
      "Epoch 57/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.2052 - accuracy: 0.4660\n",
      "Epoch 58/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.2040 - accuracy: 0.4667\n",
      "Epoch 59/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.2023 - accuracy: 0.4688\n",
      "Epoch 60/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.2008 - accuracy: 0.4703\n",
      "Epoch 61/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.1994 - accuracy: 0.4700\n",
      "Epoch 62/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 1.1980 - accuracy: 0.4694\n",
      "Epoch 63/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.1971 - accuracy: 0.4707\n",
      "Epoch 64/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.1958 - accuracy: 0.4720\n",
      "Epoch 65/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.1947 - accuracy: 0.4724\n",
      "Epoch 66/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.1927 - accuracy: 0.4732\n",
      "Epoch 67/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 1.1920 - accuracy: 0.4735\n",
      "Epoch 68/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 1.1906 - accuracy: 0.4731\n",
      "Epoch 69/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 1.1893 - accuracy: 0.4754\n",
      "Epoch 70/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.1878 - accuracy: 0.4758\n",
      "Epoch 71/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.1871 - accuracy: 0.4757\n",
      "Epoch 72/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.1857 - accuracy: 0.4747\n",
      "Epoch 73/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.1847 - accuracy: 0.4765\n",
      "Epoch 74/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.1832 - accuracy: 0.4772\n",
      "Epoch 75/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.1824 - accuracy: 0.4766\n",
      "Epoch 76/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.1809 - accuracy: 0.4772\n",
      "Epoch 77/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.1799 - accuracy: 0.4784\n",
      "Epoch 78/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.1788 - accuracy: 0.4797\n",
      "Epoch 79/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.1778 - accuracy: 0.4790\n",
      "Epoch 80/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.1767 - accuracy: 0.4794\n",
      "Epoch 81/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.1753 - accuracy: 0.4793\n",
      "Epoch 82/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.1749 - accuracy: 0.4800\n",
      "Epoch 83/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.1732 - accuracy: 0.4813\n",
      "Epoch 84/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.1722 - accuracy: 0.4815\n",
      "Epoch 85/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.1718 - accuracy: 0.4821\n",
      "Epoch 86/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.1709 - accuracy: 0.4819\n",
      "Epoch 87/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.1692 - accuracy: 0.4827\n",
      "Epoch 88/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.1682 - accuracy: 0.4859\n",
      "Epoch 89/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.1681 - accuracy: 0.4837\n",
      "Epoch 90/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.1670 - accuracy: 0.4844\n",
      "Epoch 91/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 1.1654 - accuracy: 0.4857\n",
      "Epoch 92/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.1649 - accuracy: 0.4855\n",
      "Epoch 93/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.1637 - accuracy: 0.4863\n",
      "Epoch 94/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.1632 - accuracy: 0.4857\n",
      "Epoch 95/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.1619 - accuracy: 0.4884\n",
      "Epoch 96/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.1614 - accuracy: 0.4863\n",
      "Epoch 97/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.1607 - accuracy: 0.4872\n",
      "Epoch 98/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.1600 - accuracy: 0.4877\n",
      "Epoch 99/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.1595 - accuracy: 0.4869\n",
      "Epoch 100/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1.1581 - accuracy: 0.4883\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7ab25734d0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_vec_embedding, Y_train_np, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "onSFAjjEbR-E",
    "outputId": "1eca2466-39bb-4ceb-f4b6-8d4a9d0c7ca7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 2s 2ms/step - loss: 1.1830 - accuracy: 0.4791\n",
      "Test Loss: 1.1829966306686401\n",
      "Test Accuracy: 0.4791499972343445\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test_vec_embedding, Y_test_np)\n",
    "\n",
    "print('Test Loss:', test_loss)\n",
    "print('Test Accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o9fdt7p0eqEe",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Question : What do you conclude by comparing accuracy values you obtain with those obtained with feedforward neural network models?\n",
    "Answer =>\n",
    "1. Part a) FNN model (test accuracy = 50.60%) performed slightly better than RNN (test accuracy = 47.91%)\n",
    "2. Part b) FNN model (test accuracy = 40.54%) performed was not good, RNN accuracy is better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fScDi_vmwdKV",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### b) GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KQxHyd6_14Rh",
    "outputId": "613e8a77-91c2-47ab-ce6e-52c51b5a9b49",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru (GRU)                   (None, 20)                19320     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 5)                 105       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 19,425\n",
      "Trainable params: 19,425\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# building RNN model\n",
    "from keras.layers import GRU\n",
    "model = keras.Sequential()\n",
    "model.add(GRU(20, activation='relu'))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "sgd = SGD(0.001)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=sgd, metrics=[\"accuracy\"])\n",
    "model.build(input_shape=(None, 20, 300))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h74gn-ziEPUR",
    "outputId": "404abfde-f913-42ba-d219-e7a3767a5400",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2500/2500 [==============================] - 15s 5ms/step - loss: 1.6116 - accuracy: 0.1971\n",
      "Epoch 2/100\n",
      "2500/2500 [==============================] - 14s 6ms/step - loss: 1.6098 - accuracy: 0.2077\n",
      "Epoch 3/100\n",
      "2500/2500 [==============================] - 14s 5ms/step - loss: 1.6090 - accuracy: 0.2227\n",
      "Epoch 4/100\n",
      "2500/2500 [==============================] - 14s 6ms/step - loss: 1.6085 - accuracy: 0.2233\n",
      "Epoch 5/100\n",
      "2500/2500 [==============================] - 14s 6ms/step - loss: 1.6081 - accuracy: 0.2237\n",
      "Epoch 6/100\n",
      "2500/2500 [==============================] - 14s 6ms/step - loss: 1.6077 - accuracy: 0.2243\n",
      "Epoch 7/100\n",
      "2500/2500 [==============================] - 15s 6ms/step - loss: 1.6073 - accuracy: 0.2246\n",
      "Epoch 8/100\n",
      "2500/2500 [==============================] - 15s 6ms/step - loss: 1.6069 - accuracy: 0.2260\n",
      "Epoch 9/100\n",
      "2500/2500 [==============================] - 16s 6ms/step - loss: 1.6065 - accuracy: 0.2257\n",
      "Epoch 10/100\n",
      "2500/2500 [==============================] - 15s 6ms/step - loss: 1.6061 - accuracy: 0.2263\n",
      "Epoch 11/100\n",
      "2500/2500 [==============================] - 16s 6ms/step - loss: 1.6057 - accuracy: 0.2274\n",
      "Epoch 12/100\n",
      "2500/2500 [==============================] - 16s 6ms/step - loss: 1.6053 - accuracy: 0.2279\n",
      "Epoch 13/100\n",
      "2500/2500 [==============================] - 15s 6ms/step - loss: 1.6049 - accuracy: 0.2284\n",
      "Epoch 14/100\n",
      "2500/2500 [==============================] - 15s 6ms/step - loss: 1.6045 - accuracy: 0.2290\n",
      "Epoch 15/100\n",
      "2500/2500 [==============================] - 15s 6ms/step - loss: 1.6041 - accuracy: 0.2293\n",
      "Epoch 16/100\n",
      "2500/2500 [==============================] - 16s 6ms/step - loss: 1.6037 - accuracy: 0.2307\n",
      "Epoch 17/100\n",
      "2500/2500 [==============================] - 15s 6ms/step - loss: 1.6033 - accuracy: 0.2309\n",
      "Epoch 18/100\n",
      "2500/2500 [==============================] - 15s 6ms/step - loss: 1.6028 - accuracy: 0.2312\n",
      "Epoch 19/100\n",
      "2500/2500 [==============================] - 15s 6ms/step - loss: 1.6024 - accuracy: 0.2323\n",
      "Epoch 20/100\n",
      "2500/2500 [==============================] - 16s 6ms/step - loss: 1.6019 - accuracy: 0.2329\n",
      "Epoch 21/100\n",
      "2500/2500 [==============================] - 16s 6ms/step - loss: 1.6014 - accuracy: 0.2338\n",
      "Epoch 22/100\n",
      "2500/2500 [==============================] - 19s 8ms/step - loss: 1.6009 - accuracy: 0.2345\n",
      "Epoch 23/100\n",
      "2500/2500 [==============================] - 16s 6ms/step - loss: 1.6004 - accuracy: 0.2357\n",
      "Epoch 24/100\n",
      "2500/2500 [==============================] - 15s 6ms/step - loss: 1.5999 - accuracy: 0.2358\n",
      "Epoch 25/100\n",
      "2500/2500 [==============================] - 15s 6ms/step - loss: 1.5994 - accuracy: 0.2359\n",
      "Epoch 26/100\n",
      "2500/2500 [==============================] - 15s 6ms/step - loss: 1.5989 - accuracy: 0.2366\n",
      "Epoch 27/100\n",
      "2500/2500 [==============================] - 16s 6ms/step - loss: 1.5984 - accuracy: 0.2374\n",
      "Epoch 28/100\n",
      "2500/2500 [==============================] - 15s 6ms/step - loss: 1.5978 - accuracy: 0.2386\n",
      "Epoch 29/100\n",
      "2500/2500 [==============================] - 15s 6ms/step - loss: 1.5973 - accuracy: 0.2383\n",
      "Epoch 30/100\n",
      "2500/2500 [==============================] - 15s 6ms/step - loss: 1.5967 - accuracy: 0.2398\n",
      "Epoch 31/100\n",
      "2500/2500 [==============================] - 15s 6ms/step - loss: 1.5961 - accuracy: 0.2403\n",
      "Epoch 32/100\n",
      "2500/2500 [==============================] - 14s 6ms/step - loss: 1.5955 - accuracy: 0.2410\n",
      "Epoch 33/100\n",
      "2500/2500 [==============================] - 14s 6ms/step - loss: 1.5948 - accuracy: 0.2420\n",
      "Epoch 34/100\n",
      "2500/2500 [==============================] - 14s 6ms/step - loss: 1.5941 - accuracy: 0.2430\n",
      "Epoch 35/100\n",
      "2500/2500 [==============================] - 15s 6ms/step - loss: 1.5934 - accuracy: 0.2440\n",
      "Epoch 36/100\n",
      "2500/2500 [==============================] - 15s 6ms/step - loss: 1.5926 - accuracy: 0.2455\n",
      "Epoch 37/100\n",
      "2500/2500 [==============================] - 15s 6ms/step - loss: 1.5918 - accuracy: 0.2459\n",
      "Epoch 38/100\n",
      "2500/2500 [==============================] - 15s 6ms/step - loss: 1.5910 - accuracy: 0.2472\n",
      "Epoch 39/100\n",
      "2500/2500 [==============================] - 15s 6ms/step - loss: 1.5900 - accuracy: 0.2484\n",
      "Epoch 40/100\n",
      "2500/2500 [==============================] - 16s 6ms/step - loss: 1.5890 - accuracy: 0.2506\n",
      "Epoch 41/100\n",
      "2500/2500 [==============================] - 14s 6ms/step - loss: 1.5879 - accuracy: 0.2512\n",
      "Epoch 42/100\n",
      "2500/2500 [==============================] - 16s 6ms/step - loss: 1.5867 - accuracy: 0.2527\n",
      "Epoch 43/100\n",
      "2500/2500 [==============================] - 15s 6ms/step - loss: 1.5854 - accuracy: 0.2542\n",
      "Epoch 44/100\n",
      "2500/2500 [==============================] - 14s 6ms/step - loss: 1.5838 - accuracy: 0.2561\n",
      "Epoch 45/100\n",
      "2500/2500 [==============================] - 14s 6ms/step - loss: 1.5820 - accuracy: 0.2583\n",
      "Epoch 46/100\n",
      "2500/2500 [==============================] - 14s 6ms/step - loss: 1.5798 - accuracy: 0.2605\n",
      "Epoch 47/100\n",
      "2500/2500 [==============================] - 14s 6ms/step - loss: 1.5772 - accuracy: 0.2627\n",
      "Epoch 48/100\n",
      "2500/2500 [==============================] - 14s 6ms/step - loss: 1.5736 - accuracy: 0.2661\n",
      "Epoch 49/100\n",
      "2500/2500 [==============================] - 14s 6ms/step - loss: 1.5683 - accuracy: 0.2706\n",
      "Epoch 50/100\n",
      "2500/2500 [==============================] - 14s 6ms/step - loss: 1.5585 - accuracy: 0.2754\n",
      "Epoch 51/100\n",
      "2500/2500 [==============================] - 14s 6ms/step - loss: 1.5232 - accuracy: 0.2939\n",
      "Epoch 52/100\n",
      "2500/2500 [==============================] - 15s 6ms/step - loss: 1.4260 - accuracy: 0.3551\n",
      "Epoch 53/100\n",
      "2500/2500 [==============================] - 14s 6ms/step - loss: 1.3925 - accuracy: 0.3714\n",
      "Epoch 54/100\n",
      "2500/2500 [==============================] - 14s 6ms/step - loss: 1.3725 - accuracy: 0.3805\n",
      "Epoch 55/100\n",
      "2500/2500 [==============================] - 14s 6ms/step - loss: 1.3572 - accuracy: 0.3859\n",
      "Epoch 56/100\n",
      "2500/2500 [==============================] - 14s 6ms/step - loss: 1.3449 - accuracy: 0.3897\n",
      "Epoch 57/100\n",
      "2500/2500 [==============================] - 14s 6ms/step - loss: 1.3347 - accuracy: 0.3950\n",
      "Epoch 58/100\n",
      "2500/2500 [==============================] - 14s 6ms/step - loss: 1.3264 - accuracy: 0.3984\n",
      "Epoch 59/100\n",
      "2500/2500 [==============================] - 14s 6ms/step - loss: 1.3199 - accuracy: 0.4010\n",
      "Epoch 60/100\n",
      "2500/2500 [==============================] - 14s 6ms/step - loss: 1.3143 - accuracy: 0.4029\n",
      "Epoch 61/100\n",
      "2500/2500 [==============================] - 14s 6ms/step - loss: 1.3096 - accuracy: 0.4050\n",
      "Epoch 62/100\n",
      "2500/2500 [==============================] - 14s 6ms/step - loss: 1.3055 - accuracy: 0.4081\n",
      "Epoch 63/100\n",
      "2500/2500 [==============================] - 14s 6ms/step - loss: 1.3019 - accuracy: 0.4090\n",
      "Epoch 64/100\n",
      "2500/2500 [==============================] - 14s 6ms/step - loss: 1.2988 - accuracy: 0.4120\n",
      "Epoch 65/100\n",
      "2500/2500 [==============================] - 14s 6ms/step - loss: 1.2957 - accuracy: 0.4140\n",
      "Epoch 66/100\n",
      "2500/2500 [==============================] - 14s 6ms/step - loss: 1.2928 - accuracy: 0.4139\n",
      "Epoch 67/100\n",
      "2500/2500 [==============================] - 14s 6ms/step - loss: 1.2905 - accuracy: 0.4162\n",
      "Epoch 68/100\n",
      "2500/2500 [==============================] - 14s 6ms/step - loss: 1.2881 - accuracy: 0.4180\n",
      "Epoch 69/100\n",
      "2500/2500 [==============================] - 14s 6ms/step - loss: 1.2857 - accuracy: 0.4193\n",
      "Epoch 70/100\n",
      "2500/2500 [==============================] - 14s 6ms/step - loss: 1.2833 - accuracy: 0.4207\n",
      "Epoch 71/100\n",
      "2500/2500 [==============================] - 14s 6ms/step - loss: 1.2814 - accuracy: 0.4220\n",
      "Epoch 72/100\n",
      "2500/2500 [==============================] - 14s 6ms/step - loss: 1.2791 - accuracy: 0.4226\n",
      "Epoch 73/100\n",
      "2500/2500 [==============================] - 15s 6ms/step - loss: 1.2773 - accuracy: 0.4245\n",
      "Epoch 74/100\n",
      "2500/2500 [==============================] - 14s 6ms/step - loss: 1.2751 - accuracy: 0.4255\n",
      "Epoch 75/100\n",
      "2500/2500 [==============================] - 14s 6ms/step - loss: 1.2731 - accuracy: 0.4265\n",
      "Epoch 76/100\n",
      "2500/2500 [==============================] - 14s 6ms/step - loss: 1.2710 - accuracy: 0.4286\n",
      "Epoch 77/100\n",
      "2500/2500 [==============================] - 14s 6ms/step - loss: 1.2689 - accuracy: 0.4291\n",
      "Epoch 78/100\n",
      "2500/2500 [==============================] - 15s 6ms/step - loss: 1.2668 - accuracy: 0.4309\n",
      "Epoch 79/100\n",
      "2500/2500 [==============================] - 14s 6ms/step - loss: 1.2644 - accuracy: 0.4330\n",
      "Epoch 80/100\n",
      "2500/2500 [==============================] - 14s 6ms/step - loss: 1.2620 - accuracy: 0.4344\n",
      "Epoch 81/100\n",
      "2500/2500 [==============================] - 14s 6ms/step - loss: 1.2595 - accuracy: 0.4350\n",
      "Epoch 82/100\n",
      "2500/2500 [==============================] - 14s 6ms/step - loss: 1.2570 - accuracy: 0.4359\n",
      "Epoch 83/100\n",
      "2500/2500 [==============================] - 14s 6ms/step - loss: 1.2541 - accuracy: 0.4392\n",
      "Epoch 84/100\n",
      "2500/2500 [==============================] - 14s 6ms/step - loss: 1.2510 - accuracy: 0.4411\n",
      "Epoch 85/100\n",
      "2500/2500 [==============================] - 14s 6ms/step - loss: 1.2475 - accuracy: 0.4419\n",
      "Epoch 86/100\n",
      "2500/2500 [==============================] - 14s 6ms/step - loss: 1.2441 - accuracy: 0.4450\n",
      "Epoch 87/100\n",
      "2500/2500 [==============================] - 14s 6ms/step - loss: 1.2409 - accuracy: 0.4471\n",
      "Epoch 88/100\n",
      "2500/2500 [==============================] - 14s 6ms/step - loss: 1.2378 - accuracy: 0.4484\n",
      "Epoch 89/100\n",
      "2500/2500 [==============================] - 18s 7ms/step - loss: 1.2352 - accuracy: 0.4484\n",
      "Epoch 90/100\n",
      "2500/2500 [==============================] - 14s 6ms/step - loss: 1.2327 - accuracy: 0.4509\n",
      "Epoch 91/100\n",
      "2500/2500 [==============================] - 14s 6ms/step - loss: 1.2303 - accuracy: 0.4525\n",
      "Epoch 92/100\n",
      "2500/2500 [==============================] - 14s 6ms/step - loss: 1.2281 - accuracy: 0.4541\n",
      "Epoch 93/100\n",
      "2500/2500 [==============================] - 14s 6ms/step - loss: 1.2261 - accuracy: 0.4543\n",
      "Epoch 94/100\n",
      "2500/2500 [==============================] - 15s 6ms/step - loss: 1.2242 - accuracy: 0.4560\n",
      "Epoch 95/100\n",
      "2500/2500 [==============================] - 14s 6ms/step - loss: 1.2222 - accuracy: 0.4579\n",
      "Epoch 96/100\n",
      "2500/2500 [==============================] - 14s 6ms/step - loss: 1.2204 - accuracy: 0.4595\n",
      "Epoch 97/100\n",
      "2500/2500 [==============================] - 14s 6ms/step - loss: 1.2186 - accuracy: 0.4607\n",
      "Epoch 98/100\n",
      "2500/2500 [==============================] - 14s 6ms/step - loss: 1.2169 - accuracy: 0.4615\n",
      "Epoch 99/100\n",
      "2500/2500 [==============================] - 14s 6ms/step - loss: 1.2156 - accuracy: 0.4606\n",
      "Epoch 100/100\n",
      "2500/2500 [==============================] - 14s 6ms/step - loss: 1.2142 - accuracy: 0.4622\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7aa1c46a50>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_vec_embedding, Y_train_np, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B7v1jGR-7uOD",
    "outputId": "c6d197d3-b5e9-4d8a-fcfe-156559afab84",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 2s 3ms/step - loss: 1.2230 - accuracy: 0.4593\n",
      "Test Loss: 1.223007082939148\n",
      "Test Accuracy: 0.4593000113964081\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test_vec_embedding, Y_test_np)\n",
    "\n",
    "print('Test Loss:', test_loss)\n",
    "print('Test Accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QJdgDisk7zCn",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Question: What do you conclude by comparing accuracy values you obtain with those obtained using simple RNN ?\n",
    "Answer =>\n",
    "Simple RNN (test accuracy = 47.91%) performed better on unseen test data than GRU (test accuracy = 45.93%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-SrkGHB_delS",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
